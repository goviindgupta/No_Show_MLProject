{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import layers, models\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Data Types:\n",
      " Gender                 object\n",
      "Age                     int64\n",
      "Alcohol_Consumption    object\n",
      "Hypertension             bool\n",
      "Diabetes                 bool\n",
      "Appointment_Date       object\n",
      "Schedule_Date          object\n",
      "Clinic_Location        object\n",
      "Specialty              object\n",
      "Neighborhood           object\n",
      "target_no_show          int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv('reduced_data.csv')\n",
    "\n",
    "# Check for and handle missing values (if any)\n",
    "data = data.dropna()\n",
    "\n",
    "# Convert 'target_no_show' from bool to int\n",
    "data['target_no_show'] = data['target_no_show'].astype(int)\n",
    "\n",
    "# Verify data types before processing\n",
    "print(\"Initial Data Types:\\n\", data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create age group categories and one-hot encoding\n",
    "data['age_group'] = pd.cut(data['Age'], bins=[0, 30, 40, 50, 60, 100], labels=['<30', '30-40', '40-50', '50-60', '>60'])\n",
    "data = pd.get_dummies(data, columns=['age_group'], drop_first=True)\n",
    "data.drop('Age', axis='columns', inplace=True)\n",
    "\n",
    "# Map 'Alcohol Consumption' to numeric values\n",
    "mapping_dict = {'0/week': 0, '1/week': 1, '5/week': 2, '10/week': 3, '> 14/week': 4}\n",
    "data['Alcohol_Consumption'] = data['Alcohol_Consumption'].map(mapping_dict)\n",
    "\n",
    "# Convert other necessary columns to integers\n",
    "data['Hypertension'] = data['Hypertension'].astype(int)\n",
    "data['Diabetes'] = data['Diabetes'].astype(int)\n",
    "\n",
    "# Convert date columns to datetime format and extract features\n",
    "data['Appointment_Date'] = pd.to_datetime(data['Appointment_Date'])\n",
    "data['Schedule_Date'] = pd.to_datetime(data['Schedule_Date'])\n",
    "data['days_until_appointment'] = (data['Appointment_Date'] - data['Schedule_Date']).dt.days\n",
    "\n",
    "# Drop the original date columns if not needed anymore\n",
    "data.drop(columns=['Appointment_Date', 'Schedule_Date'], inplace=True)\n",
    "\n",
    "# Apply One-Hot Encoding for nominal variables\n",
    "data = pd.get_dummies(data, columns=['Clinic_Location', 'Specialty', 'Neighborhood'])\n",
    "\n",
    "# Apply Label Encoding for binary variables\n",
    "le = LabelEncoder()\n",
    "data['Gender'] = le.fit_transform(data['Gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Types After Conversion:\n",
      " Gender                              int64\n",
      "Alcohol_Consumption                 int64\n",
      "Hypertension                        int64\n",
      "Diabetes                            int64\n",
      "target_no_show                      int64\n",
      "                                    ...  \n",
      "Neighborhood_Treasure Island/YBI    int64\n",
      "Neighborhood_Twin Peaks             int64\n",
      "Neighborhood_Visitacion Valley      int64\n",
      "Neighborhood_West of Twin Peaks     int64\n",
      "Neighborhood_Western Addition       int64\n",
      "Length: 323, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Convert boolean columns to integers\n",
    "bool_columns = data.select_dtypes(include=['bool']).columns\n",
    "for column in bool_columns:\n",
    "    data[column] = data[column].astype(int)\n",
    "\n",
    "# Verify data types after conversion\n",
    "print(\"\\nData Types After Conversion:\\n\", data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all data is numeric\n",
    "assert data.apply(lambda x: np.issubdtype(x.dtype, np.number)).all(), \"Non-numeric data found in dataset\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature and target arrays\n",
    "X = data.drop(columns=['target_no_show']).values\n",
    "y = data['target_no_show'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE to balance the classes\n",
    "smote = SMOTE(random_state=42)\n",
    "X_smote, y_smote = smote.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to TensorFlow-compatible format and ensure data type consistency\n",
    "X_smote = np.array(X_smote, dtype=np.float32)\n",
    "y_smote = np.array(y_smote, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shapes and Types of Arrays:\n",
      "X_smote type: <class 'numpy.ndarray'>, dtype: float32, shape: (53704, 322)\n",
      "y_smote type: <class 'numpy.ndarray'>, dtype: float32, shape: (53704,)\n"
     ]
    }
   ],
   "source": [
    "# Verify shapes and types of the arrays\n",
    "print(\"\\nShapes and Types of Arrays:\")\n",
    "print(f\"X_smote type: {type(X_smote)}, dtype: {X_smote.dtype}, shape: {X_smote.shape}\")\n",
    "print(f\"y_smote type: {type(y_smote)}, dtype: {y_smote.dtype}, shape: {y_smote.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_smote, y_smote, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_train type: <class 'numpy.ndarray'>, dtype: float32, shape: (42963, 322)\n",
      "y_train type: <class 'numpy.ndarray'>, dtype: float32, shape: (42963,)\n"
     ]
    }
   ],
   "source": [
    "# Print shapes and types for debugging\n",
    "print(f\"\\nX_train type: {type(X_train)}, dtype: {X_train.dtype}, shape: {X_train.shape}\")\n",
    "print(f\"y_train type: {type(y_train)}, dtype: {y_train.dtype}, shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/govindkumargupta/Documents/No_Show_MLProject/venv/lib/python3.9/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Define a simple TensorFlow model\n",
    "model = models.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6812 - loss: 0.6108 - val_accuracy: 0.8122 - val_loss: 0.4096\n",
      "Epoch 2/10\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8125 - loss: 0.4060 - val_accuracy: 0.8147 - val_loss: 0.4129\n",
      "Epoch 3/10\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8161 - loss: 0.3797 - val_accuracy: 0.8263 - val_loss: 0.3672\n",
      "Epoch 4/10\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8284 - loss: 0.3591 - val_accuracy: 0.7855 - val_loss: 0.4187\n",
      "Epoch 5/10\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8280 - loss: 0.3568 - val_accuracy: 0.8278 - val_loss: 0.3519\n",
      "Epoch 6/10\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8364 - loss: 0.3387 - val_accuracy: 0.7820 - val_loss: 0.4216\n",
      "Epoch 7/10\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8338 - loss: 0.3326 - val_accuracy: 0.8364 - val_loss: 0.3544\n",
      "Epoch 8/10\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8446 - loss: 0.3146 - val_accuracy: 0.8458 - val_loss: 0.3382\n",
      "Epoch 9/10\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8519 - loss: 0.3031 - val_accuracy: 0.8473 - val_loss: 0.3245\n",
      "Epoch 10/10\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8569 - loss: 0.2945 - val_accuracy: 0.8555 - val_loss: 0.3075\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "try:\n",
    "    history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "except Exception as e:\n",
    "    print(f\"Error during model training: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 623us/step - accuracy: 0.8616 - loss: 0.2877\n",
      "\n",
      "Test Accuracy: 0.86\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"\\nTest Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 749us/step\n"
     ]
    }
   ],
   "source": [
    "# Predict the labels for the test set\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.87\n",
      "Recall: 0.84\n",
      "F1 Score: 0.86\n",
      "Accuracy: 0.86\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print classification metrics\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Show       0.85      0.88      0.86      5407\n",
      "     No Show       0.87      0.84      0.86      5334\n",
      "\n",
      "    accuracy                           0.86     10741\n",
      "   macro avg       0.86      0.86      0.86     10741\n",
      "weighted avg       0.86      0.86      0.86     10741\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate and print the full classification report\n",
    "LABELS = [\"Show\", \"No Show\"]\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=LABELS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Data Types:\n",
      " Gender                 object\n",
      "Age                     int64\n",
      "Alcohol_Consumption    object\n",
      "Hypertension             bool\n",
      "Diabetes                 bool\n",
      "Appointment_Date       object\n",
      "Schedule_Date          object\n",
      "Clinic_Location        object\n",
      "Specialty              object\n",
      "Neighborhood           object\n",
      "target_no_show          int64\n",
      "dtype: object\n",
      "\n",
      "Data Types After Conversion:\n",
      " Gender                              int64\n",
      "Alcohol_Consumption                 int64\n",
      "Hypertension                        int64\n",
      "Diabetes                            int64\n",
      "target_no_show                      int64\n",
      "                                    ...  \n",
      "Neighborhood_Treasure Island/YBI    int64\n",
      "Neighborhood_Twin Peaks             int64\n",
      "Neighborhood_Visitacion Valley      int64\n",
      "Neighborhood_West of Twin Peaks     int64\n",
      "Neighborhood_Western Addition       int64\n",
      "Length: 323, dtype: object\n",
      "\n",
      "Shapes and Types of Arrays:\n",
      "X_smote type: <class 'numpy.ndarray'>, dtype: float32, shape: (53704, 322)\n",
      "y_smote type: <class 'numpy.ndarray'>, dtype: float32, shape: (53704,)\n",
      "\n",
      "X_train type: <class 'numpy.ndarray'>, dtype: float32, shape: (42963, 322)\n",
      "y_train type: <class 'numpy.ndarray'>, dtype: float32, shape: (42963,)\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/govindkumargupta/Documents/No_Show_MLProject/venv/lib/python3.9/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7015 - loss: 0.5970 - val_accuracy: 0.6972 - val_loss: 0.7273\n",
      "Epoch 2/10\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8050 - loss: 0.4220 - val_accuracy: 0.8181 - val_loss: 0.4136\n",
      "Epoch 3/10\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8163 - loss: 0.3832 - val_accuracy: 0.7906 - val_loss: 0.4071\n",
      "Epoch 4/10\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8219 - loss: 0.3695 - val_accuracy: 0.8281 - val_loss: 0.3744\n",
      "Epoch 5/10\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8340 - loss: 0.3434 - val_accuracy: 0.8277 - val_loss: 0.3418\n",
      "Epoch 6/10\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8377 - loss: 0.3290 - val_accuracy: 0.8455 - val_loss: 0.3279\n",
      "Epoch 7/10\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8421 - loss: 0.3211 - val_accuracy: 0.8484 - val_loss: 0.3230\n",
      "Epoch 8/10\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8477 - loss: 0.3093 - val_accuracy: 0.8347 - val_loss: 0.3303\n",
      "Epoch 9/10\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8554 - loss: 0.2963 - val_accuracy: 0.8349 - val_loss: 0.3499\n",
      "Epoch 10/10\n",
      "\u001b[1m1075/1075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8571 - loss: 0.2941 - val_accuracy: 0.8239 - val_loss: 0.3781\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 727us/step - accuracy: 0.8308 - loss: 0.3645\n",
      "\n",
      "Test Accuracy: 0.82\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 819us/step\n",
      "Precision: 0.97\n",
      "Recall: 0.67\n",
      "F1 Score: 0.79\n",
      "Accuracy: 0.82\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Show       0.75      0.98      0.85      5407\n",
      "     No Show       0.97      0.67      0.79      5334\n",
      "\n",
      "    accuracy                           0.82     10741\n",
      "   macro avg       0.86      0.82      0.82     10741\n",
      "weighted avg       0.86      0.82      0.82     10741\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    " \n",
    "# Load the data\n",
    "data = pd.read_csv('reduced_data.csv')\n",
    " \n",
    "# Check for and handle missing values (if any)\n",
    "data = data.dropna()\n",
    " \n",
    "# Convert 'target_no_show' from bool to int\n",
    "data['target_no_show'] = data['target_no_show'].astype(int)\n",
    " \n",
    "# Verify data types before processing\n",
    "print(\"Initial Data Types:\\n\", data.dtypes)\n",
    " \n",
    "# Create age group categories and one-hot encoding\n",
    "data['age_group'] = pd.cut(data['Age'], bins=[0, 30, 40, 50, 60, 100], labels=['<30', '30-40', '40-50', '50-60', '>60'])\n",
    "data = pd.get_dummies(data, columns=['age_group'], drop_first=True)\n",
    "data.drop('Age', axis='columns', inplace=True)\n",
    " \n",
    "# Map 'Alcohol Consumption' to numeric values\n",
    "mapping_dict = {'0/week': 0, '1/week': 1, '5/week': 2, '10/week': 3, '> 14/week': 4}\n",
    "data['Alcohol_Consumption'] = data['Alcohol_Consumption'].map(mapping_dict)\n",
    " \n",
    "# Convert other necessary columns to integers\n",
    "data['Hypertension'] = data['Hypertension'].astype(int)\n",
    "data['Diabetes'] = data['Diabetes'].astype(int)\n",
    " \n",
    "# Convert date columns to datetime format and extract features\n",
    "data['Appointment_Date'] = pd.to_datetime(data['Appointment_Date'])\n",
    "data['Schedule_Date'] = pd.to_datetime(data['Schedule_Date'])\n",
    "data['days_until_appointment'] = (data['Appointment_Date'] - data['Schedule_Date']).dt.days\n",
    " \n",
    "# Drop the original date columns if not needed anymore\n",
    "data.drop(columns=['Appointment_Date', 'Schedule_Date'], inplace=True)\n",
    " \n",
    "# Apply One-Hot Encoding for nominal variables\n",
    "data = pd.get_dummies(data, columns=['Clinic_Location', 'Specialty', 'Neighborhood'])\n",
    " \n",
    "# Apply Label Encoding for binary variables\n",
    "le = LabelEncoder()\n",
    "data['Gender'] = le.fit_transform(data['Gender'])\n",
    " \n",
    "# Convert boolean columns to integers\n",
    "bool_columns = data.select_dtypes(include=['bool']).columns\n",
    "for column in bool_columns:\n",
    "    data[column] = data[column].astype(int)\n",
    " \n",
    "# Verify data types after conversion\n",
    "print(\"\\nData Types After Conversion:\\n\", data.dtypes)\n",
    " \n",
    "# Ensure all data is numeric\n",
    "assert data.apply(lambda x: np.issubdtype(x.dtype, np.number)).all(), \"Non-numeric data found in dataset\"\n",
    " \n",
    "# Create feature and target arrays\n",
    "X = data.drop(columns=['target_no_show']).values\n",
    "y = data['target_no_show'].values\n",
    " \n",
    "# Apply SMOTE to balance the classes\n",
    "smote = SMOTE(random_state=42)\n",
    "X_smote, y_smote = smote.fit_resample(X, y)\n",
    " \n",
    "# Convert to TensorFlow-compatible format and ensure data type consistency\n",
    "X_smote = np.array(X_smote, dtype=np.float32)\n",
    "y_smote = np.array(y_smote, dtype=np.float32)\n",
    " \n",
    "# Verify shapes and types of the arrays\n",
    "print(\"\\nShapes and Types of Arrays:\")\n",
    "print(f\"X_smote type: {type(X_smote)}, dtype: {X_smote.dtype}, shape: {X_smote.shape}\")\n",
    "print(f\"y_smote type: {type(y_smote)}, dtype: {y_smote.dtype}, shape: {y_smote.shape}\")\n",
    " \n",
    " \n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_smote, y_smote, test_size=0.2, random_state=42)\n",
    " \n",
    "# Print shapes and types for debugging\n",
    "print(f\"\\nX_train type: {type(X_train)}, dtype: {X_train.dtype}, shape: {X_train.shape}\")\n",
    "print(f\"y_train type: {type(y_train)}, dtype: {y_train.dtype}, shape: {y_train.shape}\")\n",
    " \n",
    "# Define a simple TensorFlow model\n",
    "model = models.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    " \n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    " \n",
    "# Train the model\n",
    "try:\n",
    "    history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "except Exception as e:\n",
    "    print(f\"Error during model training: {e}\")\n",
    " \n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"\\nTest Accuracy: {accuracy:.2f}\")\n",
    " \n",
    "# Predict the labels for the test set\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    " \n",
    "# Calculate and print classification metrics\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    " \n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    " \n",
    "# Generate and print the full classification report\n",
    "LABELS = [\"Show\", \"No Show\"]\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=LABELS))\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
